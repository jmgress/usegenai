---
marp: true
theme: custom-default
footer: 'James Gress | AI Director @ Accenture | https://jmgress.github.io/futureofswe/'
---
# Future of Software Engineering in the Age of AI

![bg right:40%](img/00-jamesgress.png)

## James Gress
_Emerging Technology and AI Lead / Accenture_


<i class="fa-brands fa-linkedin"></i> LinkedIn: - [jamesgress](https://linkedin.com/in/jamesgress/)  
<i class="fa-brands fa-github"></i> GitHub: [jmgress](https://github.com/jmgress)  
<i class="fa-brands fa-x-twitter"></i> X.com: [@jmgress](https://x.com/jmgress)  
<i class="fa-brands fa-meetup"></i> [Tampa Bay Generative AI Meetup](https://www.meetup.com/tampa-bay-generative-ai-meetup/)  
<i class="fa-brands fa-meetup"></i> [Tampa Bay DevOps Meetup](https://www.meetup.com/tampa-devops-meetup/)
<i class="fa-brands fa-meetup"></i> [Tampa Bay Platform Engineering Meetup](https://www.meetup.com/tampabayplatformengineering/)
<!-- 
Done 100's of Prototypes
Taken 10 applications to Production ranging from simple RAG to more complex Agentic systems
-->
---

# What is Generative AI

Generative AI refers to artificial intelligence systems that can create new content, including text, images, code, and more.

## Key Types of Generative AI Models

<div class="columns">
<div>

### Large Language Models (LLMs)
- **Text generation and understanding**
- Examples: GPT, Claude, LLaMA
- Trained on vast amounts of text data
- Can perform various language tasks

</div>
<div>

### Other Generative Models
- **Image generation** (DALL-E, Midjourney)
- **Code generation** (GitHub Copilot)
- **Audio/Video synthesis**
- **Multimodal models** (GPT-4V, Gemini)

</div>
</div>

<!-- 
LLMs are the foundation of most modern AI applications we interact with daily.
They understand context, generate human-like text, and can be fine-tuned for specific tasks.
-->

---

# How LLMs are Created

The creation of Large Language Models involves several complex stages requiring massive computational resources and careful engineering.

## The Creation Pipeline

<div class="columns3">
<div>

### üìä Data Collection
- **Web scraping** at internet scale
- **Books, articles, code repositories**
- **Quality filtering and deduplication**
- **Ethical considerations and consent**

</div>
<div>

### üèãÔ∏è Training Process
- **Pre-training**: Learn language patterns
- **Fine-tuning**: Task-specific optimization  
- **RLHF**: Human feedback alignment
- **Optimization algorithms** (Adam, AdamW)

</div>
<div>

### üèóÔ∏è Infrastructure
- **Thousands of GPUs/TPUs**
- **Distributed computing clusters**
- **Months of training time**
- **Millions in computational costs**

</div>
</div>

### Key Challenges
- **Data quality and bias mitigation** ‚Ä¢ **Computational efficiency** ‚Ä¢ **Model alignment and safety**

<!-- 
Training LLMs requires coordination across thousands of machines, sophisticated data pipelines, and careful attention to model behavior and safety.
The entire process can cost tens of millions of dollars and requires world-class engineering expertise.
-->

---

# Getting Started